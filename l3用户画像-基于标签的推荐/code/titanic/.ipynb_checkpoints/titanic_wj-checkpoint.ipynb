{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T06:26:17.400126Z",
     "start_time": "2020-12-02T06:26:16.945829Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T06:26:17.452609Z",
     "start_time": "2020-12-02T06:26:17.429367Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "test_data = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:10.809087Z",
     "start_time": "2020-12-02T09:03:10.801789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      "PassengerId    891 non-null int64\n",
      "Survived       891 non-null int64\n",
      "Pclass         891 non-null int64\n",
      "Name           891 non-null object\n",
      "Sex            891 non-null object\n",
      "Age            891 non-null float64\n",
      "SibSp          891 non-null int64\n",
      "Parch          891 non-null int64\n",
      "Ticket         891 non-null object\n",
      "Fare           891 non-null float64\n",
      "Cabin          204 non-null object\n",
      "Embarked       891 non-null object\n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 数据探索\n",
    "# 查看train_data信息: 列名 非空个数 类型等\n",
    "print(train_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:12.100486Z",
     "start_time": "2020-12-02T09:03:12.072170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   13.002015    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   29.699118    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   35.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "# 查看数据摘要\n",
    "print(train_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:14.668959Z",
     "start_time": "2020-12-02T09:03:14.644017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Name   Sex  Ticket Cabin Embarked\n",
      "count                          891   891     891   204      891\n",
      "unique                         891     2     681   147        3\n",
      "top     Lemberopolous, Mr. Peter L  male  347082    G6        S\n",
      "freq                             1   577       7     4      646\n"
     ]
    }
   ],
   "source": [
    "# 查看离散数据分布\n",
    "# include，这个参数默认是只计算数值型特征的统计量，当输入include=['O']，会计算离散型变量的统计特征\n",
    "print(train_data.describe(include=['O']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:14.875908Z",
     "start_time": "2020-12-02T09:03:14.861122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "     PassengerId  Survived  Pclass                                      Name  \\\n",
      "886          887         0       2                     Montvila, Rev. Juozas   \n",
      "887          888         1       1              Graham, Miss. Margaret Edith   \n",
      "888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"   \n",
      "889          890         1       1                     Behr, Mr. Karl Howell   \n",
      "890          891         0       3                       Dooley, Mr. Patrick   \n",
      "\n",
      "        Sex        Age  SibSp  Parch      Ticket   Fare Cabin Embarked  \n",
      "886    male  27.000000      0      0      211536  13.00   NaN        S  \n",
      "887  female  19.000000      0      0      112053  30.00   B42        S  \n",
      "888  female  29.699118      1      2  W./C. 6607  23.45   NaN        S  \n",
      "889    male  26.000000      0      0      111369  30.00  C148        C  \n",
      "890    male  32.000000      0      0      370376   7.75   NaN        Q  \n"
     ]
    }
   ],
   "source": [
    "# 查看前 后5条数据\n",
    "print(train_data.head())\n",
    "print(train_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:16.053889Z",
     "start_time": "2020-12-02T09:03:16.042408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0500      43\n",
      "13.0000     42\n",
      "7.8958      38\n",
      "7.7500      34\n",
      "26.0000     31\n",
      "10.5000     24\n",
      "7.9250      18\n",
      "7.7750      16\n",
      "26.5500     15\n",
      "0.0000      15\n",
      "7.2292      15\n",
      "7.8542      13\n",
      "8.6625      13\n",
      "7.2500      13\n",
      "7.2250      12\n",
      "16.1000      9\n",
      "9.5000       9\n",
      "24.1500      8\n",
      "15.5000      8\n",
      "56.4958      7\n",
      "52.0000      7\n",
      "14.5000      7\n",
      "14.4542      7\n",
      "69.5500      7\n",
      "7.0500       7\n",
      "31.2750      7\n",
      "46.9000      6\n",
      "30.0000      6\n",
      "7.7958       6\n",
      "39.6875      6\n",
      "            ..\n",
      "7.1417       1\n",
      "42.4000      1\n",
      "211.5000     1\n",
      "12.2750      1\n",
      "61.1750      1\n",
      "8.4333       1\n",
      "51.4792      1\n",
      "7.8875       1\n",
      "8.6833       1\n",
      "7.5208       1\n",
      "34.6542      1\n",
      "28.7125      1\n",
      "25.5875      1\n",
      "7.7292       1\n",
      "12.2875      1\n",
      "8.6542       1\n",
      "8.7125       1\n",
      "61.3792      1\n",
      "6.9500       1\n",
      "9.8417       1\n",
      "8.3000       1\n",
      "13.7917      1\n",
      "9.4750       1\n",
      "13.4167      1\n",
      "26.3875      1\n",
      "8.4583       1\n",
      "9.8375       1\n",
      "8.3625       1\n",
      "14.1083      1\n",
      "17.4000      1\n",
      "Name: Fare, Length: 248, dtype: int64\n",
      "S    646\n",
      "C    168\n",
      "Q     77\n",
      "Name: Embarked, dtype: int64\n",
      "male      577\n",
      "female    314\n",
      "Name: Sex, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# value_counts()函数可以对Series里面的每个值进行计数并且排序\n",
    "#print(train_data['Age'].value_counts())\n",
    "print(train_data['Fare'].value_counts())\n",
    "print(train_data['Embarked'].value_counts())\n",
    "print(train_data['Sex'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:16.753740Z",
     "start_time": "2020-12-02T09:03:16.747970Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用平均年龄来填充年龄的nan值\n",
    "train_data['Age'].fillna(train_data['Age'].mean(), inplace = True)\n",
    "test_data['Age'].fillna(test_data['Age'].mean(), inplace = True)\n",
    "# 使用票价的均值填充票价中的nan值\n",
    "train_data['Fare'].fillna(train_data['Fare'].mean(),inplace=True)\n",
    "test_data['Fare'].fillna(test_data['Fare'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:17.049983Z",
     "start_time": "2020-12-02T09:03:17.044611Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用登陆最多的港口来填充登陆港口的nan值\n",
    "train_data['Embarked'].fillna('S',inplace=True)\n",
    "test_data['Embarked'].fillna('S',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:17.425959Z",
     "start_time": "2020-12-02T09:03:17.415745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征值：   Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
      "0       3    male  22.0      1      0   7.2500        S\n",
      "1       1  female  38.0      1      0  71.2833        C\n",
      "2       3  female  26.0      0      0   7.9250        S\n",
      "3       1  female  35.0      1      0  53.1000        S\n",
      "4       3    male  35.0      0      0   8.0500        S\n"
     ]
    }
   ],
   "source": [
    "# 特征选择 Cabin 由于缺失值太多，直接丢掉\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "train_features = train_data[features]\n",
    "train_labels = train_data['Survived']\n",
    "test_features = test_data[features]\n",
    "print(f'特征值：{train_features.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:18.808035Z",
     "start_time": "2020-12-02T09:03:18.805302Z"
    }
   },
   "outputs": [],
   "source": [
    "# DictVectorizer将非数字化的值数字化，比如将Embarked='S' 设为1 将Embarked != 'S' 设为0\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:20.274190Z",
     "start_time": "2020-12-02T09:03:20.254218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp']\n"
     ]
    }
   ],
   "source": [
    "dvec = DictVectorizer(sparse=False)\n",
    "#train_data.to_dict() convert the dataframe to a dictionary\n",
    "train_features = dvec.fit_transform(train_features.to_dict(orient='record'))\n",
    "print(dvec.feature_names_)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:21.003207Z",
     "start_time": "2020-12-02T09:03:20.994734Z"
    }
   },
   "outputs": [],
   "source": [
    "test_features = dvec.transform(test_features.to_dict(orient='record'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:23.383588Z",
     "start_time": "2020-12-02T09:03:23.205464Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "构造ID3决策树\n",
    "criterion：gini或者entropy,前者是基尼系数，后者是信息熵。\n",
    "两种算法差异不大对准确率无影响，信息墒运算效率低一点，因为它有对数运算.\n",
    "一般说使用默认的基尼系数”gini”就可以了，即CART算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:27.071794Z",
     "start_time": "2020-12-02T09:03:27.048325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造ID3决策树\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "# 决策树训练\n",
    "clf.fit(train_features,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:03:28.002305Z",
     "start_time": "2020-12-02T09:03:27.997640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0\n",
      " 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 0\n",
      " 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0\n",
      " 0 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
      " 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 1 1 1\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0\n",
      " 0 0 0 0 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 决策树预测\n",
    "pred_labels = clf.predict(test_features)\n",
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:04:13.646998Z",
     "start_time": "2020-12-02T09:04:13.640027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score准确率为：0.982043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method ClassifierMixin.score of DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到决策树的准确率（基于训练集）\n",
    "# Returns the mean accuracy on the given test data and labels.\n",
    "acc_decision_tree = round(clf.score(train_features,train_labels),6)\n",
    "print(f\"score准确率为：{acc_decision_tree}\")\n",
    "clf.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:04:15.581952Z",
     "start_time": "2020-12-02T09:04:15.579496Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:05:30.371691Z",
     "start_time": "2020-12-02T09:05:30.327590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树交叉验证准确率为：0.7857765860855749\n"
     ]
    }
   ],
   "source": [
    "# 使用K折交叉验证，统计决策树准确率\n",
    "test_acc_decision_tree = np.mean(cross_val_score(clf,train_features,train_labels,cv=10))\n",
    "print(f\"决策树交叉验证准确率为：{test_acc_decision_tree}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:05:32.051334Z",
     "start_time": "2020-12-02T09:05:32.018871Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:07:16.587697Z",
     "start_time": "2020-12-02T09:07:15.953324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost交叉验证准确率为：0.8205078878674386\n"
     ]
    }
   ],
   "source": [
    "# 创建XGBoost分类器\n",
    "xgboost = XGBClassifier()\n",
    "xgboost.fit(train_features,train_labels)\n",
    "predict_y=xgboost.predict(test_features)\n",
    "acc_xgboost = np.mean(cross_val_score(xgboost,train_features,train_labels,cv=10))\n",
    "print(f\"xgboost交叉验证准确率为：{acc_xgboost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:07:17.422526Z",
     "start_time": "2020-12-02T09:07:17.419900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:07:33.949355Z",
     "start_time": "2020-12-02T09:07:33.881430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr交叉验证准确率为：0.795800987402111\n"
     ]
    }
   ],
   "source": [
    "#数据集比较小，使用liblinear,数据集大使用sag或者saga\n",
    "lr = LogisticRegression(solver='liblinear',multi_class='auto')\n",
    "lr.fit(train_features,train_labels)\n",
    "predict_y=lr.predict(test_features)\n",
    "acc_lr = np.mean(cross_val_score(lr,train_features,train_labels,cv=10))\n",
    "print(f\"lr交叉验证准确率为：{acc_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:07:35.094348Z",
     "start_time": "2020-12-02T09:07:35.084306Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:07:59.533646Z",
     "start_time": "2020-12-02T09:07:59.466969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda交叉验证准确率为：0.7912430484621497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(10, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(10, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(10, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(10, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(10, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(10, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(10, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(10, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(10, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(10, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:466: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(10, 2 - 1) = 1 components.\n",
      "  ChangedBehaviorWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:472: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
      "  warnings.warn(future_msg, FutureWarning)\n",
      "/Users/wenjiali/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "lda.fit(train_features,train_labels)\n",
    "predict_y=lda.predict(test_features)\n",
    "acc_lda = np.mean(cross_val_score(lda,train_features,train_labels,cv=10))\n",
    "print(f\"lda交叉验证准确率为：{acc_lda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:08:00.865717Z",
     "start_time": "2020-12-02T09:08:00.856689Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:08:44.149224Z",
     "start_time": "2020-12-02T09:08:44.116587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naiveBayes交叉验证准确率为：0.7866981613891727\n"
     ]
    }
   ],
   "source": [
    "naiveBayes = BernoulliNB()\n",
    "naiveBayes.fit(train_features,train_labels)\n",
    "predict_y=naiveBayes.predict(test_features)\n",
    "acc_naiveBayes = np.mean(cross_val_score(naiveBayes,train_features,train_labels,cv=10))\n",
    "print(f\"naiveBayes交叉验证准确率为：{acc_naiveBayes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:09:14.730915Z",
     "start_time": "2020-12-02T09:09:14.728560Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:09:15.894564Z",
     "start_time": "2020-12-02T09:09:15.641114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm交叉验证准确率为：0.7264374077857225\n"
     ]
    }
   ],
   "source": [
    "svm = svm.SVC(kernel='rbf',C=1.0,gamma='auto')\n",
    "svm.fit(train_features,train_labels)\n",
    "predict_y=svm.predict(test_features)\n",
    "acc_svm = np.mean(cross_val_score(svm,train_features,train_labels,cv=10))\n",
    "print(f\"svm交叉验证准确率为：{acc_svm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:09:18.001730Z",
     "start_time": "2020-12-02T09:09:17.999319Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T09:09:19.008946Z",
     "start_time": "2020-12-02T09:09:18.927676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn交叉验证准确率为：0.7083591533310635\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_features,train_labels)\n",
    "predict_y=knn.predict(test_features)\n",
    "acc_knn = np.mean(cross_val_score(knn,train_features,train_labels,cv=10))\n",
    "print(f\"knn交叉验证准确率为：{acc_knn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T02:03:58.170190Z",
     "start_time": "2020-11-20T02:03:58.149140Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T02:03:58.879231Z",
     "start_time": "2020-11-20T02:03:58.765388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost交叉验证准确率为：0.7083591533310635\n"
     ]
    }
   ],
   "source": [
    "# 弱分类器\n",
    "dt_stump = DecisionTreeClassifier(max_depth=5,min_samples_leaf=1)\n",
    "dt_stump.fit(train_features,train_labels)\n",
    "adaboost = AdaBoostClassifier(base_estimator=dt_stump,n_estimators=500)\n",
    "adaboost = KNeighborsClassifier()\n",
    "adaboost.fit(train_features,train_labels)\n",
    "predict_y=adaboost.predict(test_features)\n",
    "acc_adaboost = np.mean(cross_val_score(adaboost,train_features,train_labels,cv=10))\n",
    "print(f\"adaboost交叉验证准确率为：{acc_adaboost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T02:04:00.383257Z",
     "start_time": "2020-11-20T02:04:00.128708Z"
    }
   },
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T02:45:51.687614Z",
     "start_time": "2020-11-20T02:04:00.750984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=220.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8205052722067304\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8238319064525086\tRandomForestClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=1.0, XGBClassifier__max_depth=6, XGBClassifier__min_child_weight=7, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.6000000000000001), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=8, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.8205052722067304\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8238319064525086\tRandomForestClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=1.0, XGBClassifier__max_depth=6, XGBClassifier__min_child_weight=7, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.6000000000000001), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=8, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.8205052722067304\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8283831367404701\tExtraTreesClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=11, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=6, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.8205052722067304\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8283831367404701\tExtraTreesClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=11, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=6, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "-3\t0.8317413691175964\tLogisticRegression(GaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.7500000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)), LogisticRegression__C=1.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.8272281558855333\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=8, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8283831367404701\tExtraTreesClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=11, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=6, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "-3\t0.8317413691175964\tLogisticRegression(GaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.7500000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100)), LogisticRegression__C=1.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.8272281558855333\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=8, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.834013881190317\tLogisticRegression(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100), LogisticRegression__C=20.0, LogisticRegression__dual=False, LogisticRegression__penalty=l2)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.8272281558855333\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=8, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8362863223356941\tLogisticRegression(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100), LogisticRegression__C=1.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.8328587584806938\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8362863223356941\tLogisticRegression(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100), LogisticRegression__C=1.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.8328587584806938\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8362863223356941\tLogisticRegression(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100), LogisticRegression__C=1.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.8362043657907494\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8362863223356941\tLogisticRegression(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=13, RandomForestClassifier__n_estimators=100), LogisticRegression__C=1.0, LogisticRegression__dual=False, LogisticRegression__penalty=l1)\n",
      "\n",
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=220.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8452439548419861\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=1.0, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.8464705652564554\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=1.0, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=14, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.8477284268916755\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.9500000000000001, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=14, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.8477284268916755\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.9500000000000001, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=14, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.8477284268916755\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.9500000000000001, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=14, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.8477284268916755\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.9500000000000001, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=14, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.8477284268916755\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.9500000000000001, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=14, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.8477284268916755\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.9500000000000001, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=14, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.8477284268916755\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.9500000000000001, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=14, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.8477284268916755\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.9500000000000001, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=14, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=220.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8229503105590062\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.8267158385093168\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.001, LogisticRegression__dual=False, LogisticRegression__penalty=l1), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.8229503105590062\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.8267158385093168\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.001, LogisticRegression__dual=False, LogisticRegression__penalty=l1), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [10:11:48] src/learner.cc:723: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
      "Stack trace:\n",
      "  [bt] (0) 1   libxgboost.dylib                    0x0000000127060319 dmlc::LogMessageFatal::~LogMessageFatal() + 57\n",
      "  [bt] (1) 2   libxgboost.dylib                    0x0000000127065636 xgboost::LearnerImpl::LazyInitModel() + 806\n",
      "  [bt] (2) 3   libxgboost.dylib                    0x000000012707b06e XGBoosterUpdateOneIter + 158\n",
      "  [bt] (3) 4   libffi.6.dylib                      0x000000010c270884 ffi_call_unix64 + 76\n",
      "  [bt] (4) 5   ???                                 0x00007ffee5102550 0x0 + 140732741461328\n",
      "\n",
      ".\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.8229503105590062\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.8267158385093168\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.001, LogisticRegression__dual=False, LogisticRegression__penalty=l1), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.8229503105590062\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.8267158385093168\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.001, LogisticRegression__dual=False, LogisticRegression__penalty=l1), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.8229503105590062\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.8267158385093168\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.001, LogisticRegression__dual=False, LogisticRegression__penalty=l1), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.8229503105590062\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.8279580745341615\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.001, LogisticRegression__dual=False, LogisticRegression__penalty=l1), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=1.0, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.8229503105590062\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.8279580745341615\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.001, LogisticRegression__dual=False, LogisticRegression__penalty=l1), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=1.0, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.8254270186335404\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=1.0, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.8279580745341615\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.001, LogisticRegression__dual=False, LogisticRegression__penalty=l1), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=1.0, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.8254425465838509\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.7000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "-2\t0.8279580745341615\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.001, LogisticRegression__dual=False, LogisticRegression__penalty=l1), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=1.0, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.8291847826086955\tGradientBoostingClassifier(MultinomialNB(GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=1.0, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=19, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.3), MultinomialNB__alpha=1.0, MultinomialNB__fit_prior=True), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=5, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.8254425465838509\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.7000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9000000000000001)\n",
      "-2\t0.8279580745341615\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.001, LogisticRegression__dual=False, LogisticRegression__penalty=l1), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=1.0, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.8329270186335404\tGradientBoostingClassifier(MultinomialNB(GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=1.0, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=19, GradientBoostingClassifier__min_samples_split=4, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.3), MultinomialNB__alpha=1.0, MultinomialNB__fit_prior=True), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=220.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8191820481268799\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.835416813156764\tExtraTreesClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=5, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.55), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=6, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.8191820481268799\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.835416813156764\tExtraTreesClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=5, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.55), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=6, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.8191820481268799\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.835416813156764\tExtraTreesClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=5, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.55), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=6, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.8191820481268799\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.835416813156764\tExtraTreesClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=5, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.55), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=6, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.8354403980624243\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8367527051837962\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.835463787647955\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8367527051837962\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.835463787647955\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.837932634087269\tExtraTreesClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=5, XGBClassifier__min_child_weight=4, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.8), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.2, ExtraTreesClassifier__min_samples_leaf=6, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.835463787647955\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8441516758467127\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.835463787647955\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8441516758467127\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.8454251630923082\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=220.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8179813664596273\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.05, RandomForestClassifier__min_samples_leaf=4, RandomForestClassifier__min_samples_split=7, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.8179813664596273\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.05, RandomForestClassifier__min_samples_leaf=4, RandomForestClassifier__min_samples_split=7, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.8179813664596273\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.05, RandomForestClassifier__min_samples_leaf=4, RandomForestClassifier__min_samples_split=7, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.822973602484472\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.7000000000000001, GradientBoostingClassifier__min_samples_leaf=12, GradientBoostingClassifier__min_samples_split=5, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.45)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.8279891304347826\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=19, GradientBoostingClassifier__min_samples_split=10, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.8292158385093167\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=10, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.8292158385093167\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=10, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001)\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.8292158385093167\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=10, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.8292158385093167\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=10, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001)\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.8292158385093167\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.8500000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=10, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001)\n",
      "\n",
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=220.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8142624223602484\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=6, XGBClassifier__min_child_weight=11, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=1.0)\n",
      "-2\t0.8192857142857143\tRandomForestClassifier(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=15, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.8142624223602484\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=6, XGBClassifier__min_child_weight=11, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=1.0)\n",
      "-2\t0.8192857142857143\tRandomForestClassifier(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=15, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.8192701863354037\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=15, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8204968944099379\tRandomForestClassifier(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=15, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.8192701863354037\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=15, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.8205357142857143\tRandomForestClassifier(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=20, RandomForestClassifier__n_estimators=100)\n",
      "-3\t0.8229891304347827\tRandomForestClassifier(StandardScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=15, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.8230046583850932\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=6, XGBClassifier__min_child_weight=8, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=1.0)\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.8230046583850932\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=6, XGBClassifier__min_child_weight=8, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=1.0)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.8230046583850932\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=6, XGBClassifier__min_child_weight=8, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=1.0)\n",
      "-2\t0.8242546583850932\tRandomForestClassifier(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=20, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.8242391304347827\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=9, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.825527950310559\tRandomForestClassifier(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.25, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=20, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.8242391304347827\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=9, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.825527950310559\tRandomForestClassifier(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.25, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=20, RandomForestClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 The condensed distance matrix must contain only finite values..\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.8242391304347827\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=9, RandomForestClassifier__n_estimators=100)\n",
      "-2\t0.825527950310559\tRandomForestClassifier(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.25, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=20, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=220.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8230201863354039\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.8230201863354039\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.8230201863354039\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.8230201863354039\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.8230201863354039\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.8230201863354039\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.8230201863354039\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "-3\t0.8242391304347827\tRandomForestClassifier(XGBClassifier(MaxAbsScaler(input_matrix), XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.8500000000000001), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7000000000000001, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=20, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.8230201863354039\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "-2\t0.8242624223602484\tGradientBoostingClassifier(MaxAbsScaler(input_matrix), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=2, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.8230201863354039\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "-2\t0.8242779503105592\tGradientBoostingClassifier(MaxAbsScaler(input_matrix), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.8242624223602484\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "-2\t0.8242779503105592\tGradientBoostingClassifier(MaxAbsScaler(input_matrix), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.5, GradientBoostingClassifier__min_samples_leaf=1, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5)\n",
      "-3\t0.825473602484472\tRandomForestClassifier(ExtraTreesClassifier(XGBClassifier(input_matrix, XGBClassifier__learning_rate=0.01, XGBClassifier__max_depth=2, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.8500000000000001), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.35000000000000003, ExtraTreesClassifier__min_samples_leaf=17, ExtraTreesClassifier__min_samples_split=20, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7000000000000001, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=20, RandomForestClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=220.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8254790226180709\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=2, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.25)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.8254790226180709\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=2, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.25)\n",
      "-2\t0.8266588538614789\tExtraTreesClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.05, ExtraTreesClassifier__min_samples_leaf=12, ExtraTreesClassifier__min_samples_split=19, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.8254790226180709\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=2, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.25)\n",
      "-2\t0.8266667154966992\tGradientBoostingClassifier(ZeroCount(input_matrix), GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=2, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.25)\n",
      "-3\t0.8279556330325404\tLinearSVC(Nystroem(GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=2, GradientBoostingClassifier__max_features=0.4, GradientBoostingClassifier__min_samples_leaf=11, GradientBoostingClassifier__min_samples_split=11, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.5), Nystroem__gamma=0.9500000000000001, Nystroem__kernel=linear, Nystroem__n_components=10), LinearSVC__C=0.1, LinearSVC__dual=False, LinearSVC__loss=squared_hinge, LinearSVC__penalty=l1, LinearSVC__tol=0.0001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.8366669108168289\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8500000000000001, RandomForestClassifier__min_samples_leaf=3, RandomForestClassifier__min_samples_split=10, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.8366669108168289\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8500000000000001, RandomForestClassifier__min_samples_leaf=3, RandomForestClassifier__min_samples_split=10, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.8367059260127349\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=2, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.8367059260127349\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=2, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.8367059260127349\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=2, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.8367059260127349\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=10, GradientBoostingClassifier__min_samples_split=2, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.8428781886011173\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.6000000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=6, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.25)\n",
      "\n",
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=220.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8254867865932262\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=14, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.831658853861479\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=17, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.831658853861479\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=17, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.831658853861479\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=17, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.831658853861479\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=17, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.831658853861479\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=17, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.8429327317473339\tRandomForestClassifier(StandardScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.8429327317473339\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.8429327317473339\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.8429327317473339\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.8429327317473339\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.6000000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=220.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8342468944099378\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=1.0)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.8342468944099378\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=1.0)\n",
      "-2\t0.834254658385093\tXGBClassifier(MinMaxScaler(input_matrix), XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9000000000000001)\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.8404813664596272\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [10:41:05] src/learner.cc:723: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
      "Stack trace:\n",
      "  [bt] (0) 1   libxgboost.dylib                    0x0000000127060319 dmlc::LogMessageFatal::~LogMessageFatal() + 57\n",
      "  [bt] (1) 2   libxgboost.dylib                    0x0000000127065636 xgboost::LearnerImpl::LazyInitModel() + 806\n",
      "  [bt] (2) 3   libxgboost.dylib                    0x000000012707b06e XGBoosterUpdateOneIter + 158\n",
      "  [bt] (3) 4   libffi.6.dylib                      0x000000010c270884 ffi_call_unix64 + 76\n",
      "  [bt] (4) 5   ???                                 0x00007ffee5102550 0x0 + 140732741461328\n",
      "\n",
      ".\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.8404813664596272\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.8404813664596272\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.8404813664596272\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.8404813664596272\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.8404813664596272\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 [10:42:00] src/learner.cc:723: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?\n",
      "Stack trace:\n",
      "  [bt] (0) 1   libxgboost.dylib                    0x0000000127060319 dmlc::LogMessageFatal::~LogMessageFatal() + 57\n",
      "  [bt] (1) 2   libxgboost.dylib                    0x0000000127065636 xgboost::LearnerImpl::LazyInitModel() + 806\n",
      "  [bt] (2) 3   libxgboost.dylib                    0x000000012707b06e XGBoosterUpdateOneIter + 158\n",
      "  [bt] (3) 4   libffi.6.dylib                      0x000000010c270884 ffi_call_unix64 + 76\n",
      "  [bt] (4) 5   ???                                 0x00007ffee5102550 0x0 + 140732741461328\n",
      "\n",
      ".\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.8404813664596272\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.8404813664596272\tXGBClassifier(input_matrix, XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "-3\t0.8429425465838509\tXGBClassifier(OneHotEncoder(ZeroCount(input_matrix), OneHotEncoder__minimum_fraction=0.2, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=220.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.8191739911715301\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.7000000000000001, GradientBoostingClassifier__min_samples_leaf=9, GradientBoostingClassifier__min_samples_split=14, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.8304243818117895\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=6, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.8304243818117895\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.45, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=6, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.8354555353724755\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.8354555353724755\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.8354555353724755\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True.\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.8354555353724755\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.8378932282511036\tGradientBoostingClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.15000000000000002, RandomForestClassifier__min_samples_leaf=15, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.7000000000000001, GradientBoostingClassifier__min_samples_leaf=9, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.8354555353724755\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.8378932282511036\tGradientBoostingClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.15000000000000002, RandomForestClassifier__min_samples_leaf=15, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.7000000000000001, GradientBoostingClassifier__min_samples_leaf=9, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.8354555353724755\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.8378932282511036\tGradientBoostingClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.15000000000000002, RandomForestClassifier__min_samples_leaf=15, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.7000000000000001, GradientBoostingClassifier__min_samples_leaf=9, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9500000000000001)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.8354555353724755\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.8378932282511036\tGradientBoostingClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.15000000000000002, RandomForestClassifier__min_samples_leaf=15, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=8, GradientBoostingClassifier__max_features=0.7000000000000001, GradientBoostingClassifier__min_samples_leaf=9, GradientBoostingClassifier__min_samples_split=17, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.9500000000000001)\n",
      "\n",
      "tpot交叉验证准确率为：0.8250062421972535\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=10,population_size=20,verbosity=3)\n",
    "tpot.fit(train_features,train_labels)\n",
    "predict_y = tpot.predict(test_features)\n",
    "acc_tpot = np.mean(cross_val_score(tpot,train_features,train_labels,cv=10))\n",
    "print(f\"tpot交叉验证准确率为：{acc_tpot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T02:57:01.316207Z",
     "start_time": "2020-11-20T02:57:01.312435Z"
    }
   },
   "outputs": [],
   "source": [
    "tpot.export('tpot_titanic_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
